{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"07-2_심층신경망.ipynb","provenance":[],"authorship_tag":"ABX9TyMLqUt9eCrY6899D/SoenRg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 2개의 층을 갖는 심층 신경망 만들기"],"metadata":{"id":"z3QwI1rlWFaV"}},{"cell_type":"markdown","source":["일단 데이터를 가져와서 분리하는 것은 07-1과 같다."],"metadata":{"id":"zgHVs-s9WH6_"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0whEywrJ1Sd9","executionInfo":{"status":"ok","timestamp":1643125385182,"user_tz":-540,"elapsed":4573,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"543fdf7b-43b4-4de4-8988-4a98bfd70118"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","40960/29515 [=========================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","26435584/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","16384/5148 [===============================================================================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n","4431872/4422102 [==============================] - 0s 0us/step\n"]}],"source":["from tensorflow import keras\n","from sklearn.model_selection import train_test_split\n","\n","(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()\n","train_scaled = train_input / 255.0\n","train_scaled = train_scaled.reshape(-1, 28*28)\n","train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","source":["이제 입력층과 출력층 사이에 은닉층을 하나 추가해보자.\n","\n","모든 은닉층은 활성화 함수를 지닌다.\n","- 출력층의 활성화 함수는 종류가 제한되어 있다. (시그모이드, 소프트맥스)\n","- 은닉층의 활성화 함수는 비교적 자유롭다. (시그모이드, 렐루 등)\n","\n","은닉층에 활성화 함수를 적용하는 이유\n","- 두 개의 선형 방정식(뉴런)을 하나로 합치면, 중간 매개변수의 의미가 사라진다.\n","- $ 4a + 2 = b $, $ 3b - 5 = c $ ----> $ 12a + 1 = c $\n","- 은닉층에서 선형적인 산술 계산만 수행한다면 수행 역할이 없는 셈\n","    - 따라서 선형 계산을 적당하게 비선형적으로 비틀어 주어야 한다.\n","    - 그래서 활성화 함수를 적용하고 그래야 나름의 역할을 할 수 있다.\n","- 많이 적용하는 활성화 함수는 시그모이드\n","\n","이렇게 시그모이드를 거치는 은닉층과, 소프트맥스를 거치는 출력층을 Dense 클래스로 만들어보자.\n","- 첫 층은 반드시 input_shape로 입력 크기를 지정해주어야 한다."],"metadata":{"id":"0oFX-Y5_Wgvb"}},{"cell_type":"code","source":["dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784, ))\n","dense2 = keras.layers.Dense(10, activation='softmax')"],"metadata":{"id":"LgxTjyXMYQgk","executionInfo":{"status":"ok","timestamp":1643125385182,"user_tz":-540,"elapsed":5,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["은닉층에서 몇개의 뉴런을 사용할지는 특별한 기준이 없다. 이를 판단하기 위해서는 상당한 경험이 필요하다.\n","\n","단, 적어도 출력층의 뉴런보다는 많아야 한다. 클래스 10개에 대한 확률을 예측해야 하는데, 은닉층에 10개보다 적은 뉴런이 있다면 부족한 정보가 전달될 것.\n","\n","이제 Sequential 클래스에 위 Dense 객체들을 추가하여 심층 신경망을 만들자."],"metadata":{"id":"iSXjge-BYpST"}},{"cell_type":"code","source":["model = keras.Sequential([dense1, dense2]) # 여러 모델을 리스트로 묶어 전달"],"metadata":{"id":"101d7WE_ZhjP","executionInfo":{"status":"ok","timestamp":1643125385182,"user_tz":-540,"elapsed":4,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["케라스는 모델의 summary() 함수를 통해 층에 대한 유용한 정보를 얻을 수 있다."],"metadata":{"id":"ZJCA5NuJhaWR"}},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hEof3RdliZTV","executionInfo":{"status":"ok","timestamp":1643125385183,"user_tz":-540,"elapsed":5,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"7bdff24c-2f45-46db-8119-8f3ff37f7205"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense (Dense)               (None, 100)               78500     \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                1010      \n","                                                                 \n","=================================================================\n","Total params: 79,510\n","Trainable params: 79,510\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["레이어의 이름은 층을 만들 때 name 변수로 지정할 수 있다.\n","\n","출력 크기가 (Npne, 10) 으로\n","- 첫 번째 차원은 샘플의 개수. 왜 None 일까?\n","    - 샘플 개수가 아직 정의되지 않았기 때문\n","    - fit() 는 훈련 데이터를 잘게 나누어 여러 번에 걸쳐 경사 하강법 단계를 수행한다. (미니배치 경사 하강법)\n","        - 이 값은 fit() 에서 batch_size 변수로 바꿀 수 있다.\n","    - 샘플 개수를 고정하지 않고 어떤 배치 크기에도 유연하게 대응할 수 있도록 None으로 설정한다.\n","    - 이렇게 신경망 층에 입력되거나 출력되는 배열의 첫 차원을 배치 차원이라고 한다.\n","- 두 번째 차원은 뉴런(출력) 개수\n","\n","모델 파라미터 개수\n","- 첫째 dense는 100개의 출력에 대한 784개의 입력 + bias = (784 + 1) * 100 = 78500\n","- 둘째 dense는 10개의 출력에 대한 100개의 입력 + bias = (100 + 1) * 10 = 1010\n","\n","Non-trainable params: 경사 하강법으로 훈련되지 않는 파라미터의 개수"],"metadata":{"id":"osW5g38siqhw"}},{"cell_type":"code","source":["model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')\n","model.fit(train_scaled, train_target, epochs=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q2HvnwCYlNIa","executionInfo":{"status":"ok","timestamp":1643125404869,"user_tz":-540,"elapsed":19689,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"4a845d1f-013d-47cf-a407-52fa60641311"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1500/1500 [==============================] - 6s 4ms/step - loss: 0.5623 - accuracy: 0.8073\n","Epoch 2/5\n","1500/1500 [==============================] - 4s 3ms/step - loss: 0.4087 - accuracy: 0.8533\n","Epoch 3/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3729 - accuracy: 0.8658\n","Epoch 4/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3505 - accuracy: 0.8718\n","Epoch 5/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3334 - accuracy: 0.8791\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fe23c89a0d0>"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["추가된 층이 성능을 약간 향상시켰다."],"metadata":{"id":"BjL95G1rljTd"}},{"cell_type":"markdown","source":["## 층을 추가하는 다른 방법"],"metadata":{"id":"F8DdNdQTj_SX"}},{"cell_type":"markdown","source":["1. Sequential 생성자 호출과 동시에 Dense 객체를 정의하여 전달하기"],"metadata":{"id":"3JYHkzxqkF5X"}},{"cell_type":"code","source":["model = keras.Sequential([\n","    keras.layers.Dense(100, activation='sigmoid', input_shape=(784, ), name='hidden'),\n","    keras.layers.Dense(10, activation='softmax', name='output')\n","], name='패션 MNIST 모델')\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wdShMFdTkSR0","executionInfo":{"status":"ok","timestamp":1643125404869,"user_tz":-540,"elapsed":5,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"6ba04f56-05e6-4988-8e88-371e4c394096"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"패션 MNIST 모델\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," hidden (Dense)              (None, 100)               78500     \n","                                                                 \n"," output (Dense)              (None, 10)                1010      \n","                                                                 \n","=================================================================\n","Total params: 79,510\n","Trainable params: 79,510\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["2. Sequential 클래스의 add() 함수를 호출하기"],"metadata":{"id":"CaM1-c1LkMcK"}},{"cell_type":"code","source":["model = keras.Sequential(name = '패션 MNIST 모델')\n","model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784, ), name='hidden'))\n","model.add(keras.layers.Dense(10, activation='softmax', name='output'))\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-4hdU-gTktqg","executionInfo":{"status":"ok","timestamp":1643125405330,"user_tz":-540,"elapsed":464,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"ea750a56-73de-4afd-8501-545336f5a9f2"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"패션 MNIST 모델\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," hidden (Dense)              (None, 100)               78500     \n","                                                                 \n"," output (Dense)              (None, 10)                1010      \n","                                                                 \n","=================================================================\n","Total params: 79,510\n","Trainable params: 79,510\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["# 렐루 함수"],"metadata":{"id":"r6dZk_wLlAXy"}},{"cell_type":"markdown","source":["시그모이드의 단점\n","- 입력값의 크기가 조금만 커져도 1이나 0에 가까워지기 때문에,\n","- 은닉층이 많아질수록 극단적으로 0이나 1에 가까워지기 쉽다.\n","\n","렐루(ReLU)\n","- $ ReLU(z) = max(0, z)$\n","- 입력이 양수이면 그 양수를 그대로 출력하고, 음수이면 0으로 만든다.\n","- 렐루 함수는 특히 이미지 처리에서 좋은 성능을 낸다."],"metadata":{"id":"Hb1GgFShlqm1"}},{"cell_type":"markdown","source":["또한, 앞선 예제에서는 28x28 이미지를 784 크기의 1차원으로 직접 펼친 후 Dense의 입력층으로 넣었지만,\n","\n","케라스의 Flatten 클래스를 이용하면, 배치 차원을 제외하고 나머지 입력 차원을 모두 일렬로 펼쳐준다.\n","\n","얘도 모델에 마치 층처럼 들어가지만, 학습을 위해 기여하는 건 없다."],"metadata":{"id":"Em3LJKTOmfD9"}},{"cell_type":"code","source":["model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=(28, 28))) # 첫 층이니까 input_shape 지정\n","model.add(keras.layers.Dense(100, activation='relu')) # 활성화 함수로 렐루\n","model.add(keras.layers.Dense(10, activation='softmax'))\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lQzC3KBaoSru","executionInfo":{"status":"ok","timestamp":1643125405330,"user_tz":-540,"elapsed":3,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"319cdf80-7c1e-483c-bf08-4c48a1442bf7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten (Flatten)           (None, 784)               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 100)               78500     \n","                                                                 \n"," dense_3 (Dense)             (None, 10)                1010      \n","                                                                 \n","=================================================================\n","Total params: 79,510\n","Trainable params: 79,510\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["flatten 층의 출력 형태를 보면, 입력 데이터의 차원을 짐작할 수 있는 것도 장점.\n","\n","이제 훈련 데이터를 다시 준비해서 모델을 훈련해보자."],"metadata":{"id":"avwyXyONoqlr"}},{"cell_type":"code","source":["(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()\n","train_scaled = train_input / 255.0\n","# train_scaled = train_scaled.reshape(-1, 28*28) # flatten이 있으니 reshape는 생략한다.\n","train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)"],"metadata":{"id":"giLdalV3pEgC","executionInfo":{"status":"ok","timestamp":1643125405642,"user_tz":-540,"elapsed":314,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')\n","model.fit(train_scaled, train_target, epochs=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EhnnihdQpOnK","executionInfo":{"status":"ok","timestamp":1643125421642,"user_tz":-540,"elapsed":16001,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"bd471a4e-4218-4b64-9968-a5ced0301214"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.5356 - accuracy: 0.8106\n","Epoch 2/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3931 - accuracy: 0.8591\n","Epoch 3/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3559 - accuracy: 0.8717\n","Epoch 4/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3352 - accuracy: 0.8796\n","Epoch 5/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3170 - accuracy: 0.8865\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fe23c887c10>"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["정말 소소하게 성능이 올랐다. 이제 검증 데이터로 평가를 해보자"],"metadata":{"id":"GuymRHLMpVLi"}},{"cell_type":"code","source":["model.evaluate(val_scaled, val_target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ISIHImeGpYg1","executionInfo":{"status":"ok","timestamp":1643125422093,"user_tz":-540,"elapsed":453,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"0789a201-947e-4e12-f575-a8921f73221d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["375/375 [==============================] - 1s 1ms/step - loss: 0.3659 - accuracy: 0.8781\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.36592772603034973, 0.878083348274231]"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["# 옵티마이저"],"metadata":{"id":"L6Y-LtbApkDX"}},{"cell_type":"markdown","source":["신경망 모델 구성에는 정말 다양한 하이퍼파라미터가 필요하다.\n","- 추가할 은닉층의 개수 및 층의 종류\n","- 뉴런 개수\n","- 활성화 함수\n","- 배치 사이즈\n","- 에포크\n","- 옵티마이저(경사 하강법 알고리즘) 및 그의 학습률\n","\n","여기서는 여러가지 옵티마이저를 테스트해보자."],"metadata":{"id":"yvzIGgJ5pmqF"}},{"cell_type":"markdown","source":["SGD: 확률적 경사 하강법. 가장 기본적인 옵티마이저.\n","- 이름은 SGD 이지만, 샘플을 하나씩 뽑지 않고, 기본적으로 미니배치를 사용한다.\n","- compile() 함수의 optimizer 매개변수를 'sgd'로 지정한다.\n"],"metadata":{"id":"BGVU1J-PqTb4"}},{"cell_type":"code","source":["model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics='accuracy')\n","\n","# 위 코드는 다음 코드와 정확히 일치한다.\n","#\n","#    sgd = keras.optimizers.SGD(learning_rate=0.1) # 학습률을 변경하고 싶다면\n","#    model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy')\n","#\n","# 원래는 위와 같이 SGD 객체를 만들어야 하지만,\n","# 번거로움을 피하고자 optimizer='sgd'를 바로 넣을 수 있게 만들어져 있다."],"metadata":{"id":"8BK-yNFrq5bi","executionInfo":{"status":"ok","timestamp":1643125422451,"user_tz":-540,"elapsed":359,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["SGD 외의 자주 사용하는 옵티마이저는 다음과 같다.\n","\n","기본 경사 하강법 옵티마이저\n","- SGD 클래스의 momentum 매개변수의 기본값으 0\n","- 모멘텀 최적화(momentum optimization): 이전의 그래디언트를 마치 가속도처럼 사용하는 것\n","- Momentum: SGD에 모멘텀을 적용\n","    - SGD 클래스의 momentum 매개변수를 보통 0.9 이상으로 지정한다.\n","- Nesterov momentum: 모멘텀 최적화를 2번 반복하여 구현한다.\n","    - SGD 클래스의 nesterov 매개변수를 True로 지정\n","\n","적응적 학습률 옵티마이저\n","- 모델이 최적점에 가까이 갈수록 학습률을 낮추는 것 --> 안정적으로 최적점에 수렴할 수 있음\n","    - 이러한 학습률을 적응적 학습률(adaptive learning rate)이라고 한다.\n","    - 학습률 매개변수를 튜닝하는 수고를 덜 수 있는 것이 장점.\n","- RMSprop과 Adagrad가 있으며 compile() 메소드의 optimizer 매개변수에서 지정 가능하다.\n","    - RMSprop: optimizer 매개변수의 기본값 'rmsprop'\n","- Adam: 모멘텀 최적화와 RMSprop의 장점을 접목한 것\n","    - Adam 클래스도 keras.optimizers 패키지 아래에 있다.\n","- 위 세 클래스는 모두 learning_rate 기본값으로 0.001을 사용한다."],"metadata":{"id":"QaY44fRAr1h7"}},{"cell_type":"code","source":["sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True) # Nesterov momentum 옵티마이저 객체\n","\n","adagrad = keras.optimizers.Adagrad() # Adargad 옵티마이저 객체\n","model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics='accuracy')\n","\n","rmsprop = keras.optimizers.RMSprop() # RMSprop 옵티마이저 객체\n","model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics='accuracy')"],"metadata":{"id":"dTDThV-arnMN","executionInfo":{"status":"ok","timestamp":1643126604934,"user_tz":-540,"elapsed":284,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["여기에서는 Adam 클래스의 매개변수 기본값을 사용해 패션 MNIST 모델을 훈련해보자.\n","(모델 생성부터)"],"metadata":{"id":"RVCfJ-mi-kmw"}},{"cell_type":"code","source":["model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=(28, 28)))\n","model.add(keras.layers.Dense(100, activation='relu'))\n","model.add(keras.layers.Dense(10, activation='softmax'))"],"metadata":{"id":"f3qEhzz--rNm","executionInfo":{"status":"ok","timestamp":1643126714115,"user_tz":-540,"elapsed":481,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["compile() 함수의 optimizer를 'adam'으로 설정하고 5번의 에포크 동안 훈련하자."],"metadata":{"id":"YM1REdQY_A48"}},{"cell_type":"code","source":["model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')\n","model.fit(train_scaled, train_target, epochs=5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dqu7yDF-_Hzo","executionInfo":{"status":"ok","timestamp":1643126801746,"user_tz":-540,"elapsed":13358,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"15f915f9-2a7b-4236-9980-4a751edf39a1"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.5258 - accuracy: 0.8183\n","Epoch 2/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3915 - accuracy: 0.8595\n","Epoch 3/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3512 - accuracy: 0.8739\n","Epoch 4/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3249 - accuracy: 0.8810\n","Epoch 5/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3058 - accuracy: 0.8890\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fe23c19d7d0>"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["그냥 렐루를 사용한 RMSprop보다 소소하게 성능이 올라갔다.\n","\n","검증 세트에 대해서도 평가해보자."],"metadata":{"id":"K7ZNST33_U5Y"}},{"cell_type":"code","source":["model.evaluate(val_scaled, val_target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v_4AmR5y_f4-","executionInfo":{"status":"ok","timestamp":1643126859903,"user_tz":-540,"elapsed":742,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"0037cfe2-be9b-4487-9289-ca78d271c353"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["375/375 [==============================] - 1s 1ms/step - loss: 0.3466 - accuracy: 0.8748\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.3465598523616791, 0.874833345413208]"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["책에선 RMSprop보다 조금 높은 성능이었는데, 여기선 더 낮다. 왜그런지는 몰?루"],"metadata":{"id":"X9nGY5Nd_mYF"}}]}