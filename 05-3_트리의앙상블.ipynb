{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05-3_트리의앙상블.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO7BaPxt/8gMRH6Q4UQo2jA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["정형 데이터: 구조화하기 쉬운 수치와 같은 데이터\n","비정형 데이터: 구조화하기 어려운 텍스트, 이미지, 음성과 같은 데이터\n","\n","정형 데이터를 다루는 머신러닝 알고리즘의 끝판왕\n","--> 결정 트리 기반의 앙상블 러닝(Ensemble Learning)\n","\n","비정형 데이터는 규칙성을 찾기 어려워, 전통적인 머신러닝 방법으로는 모델을 만들기 까다롭다.\n","--> 신경망 알고리즘(딥러닝)\n","\n","이 장에서는 앙상블 학습 알고리즘에 대해 알아보자."],"metadata":{"id":"hxmWMqipKB6e"}},{"cell_type":"markdown","source":["# 랜덤 포레스트"],"metadata":{"id":"0pDEYNldKdzb"}},{"cell_type":"markdown","source":["랜덤 포레스트는\n","- 결정 트리를 랜덤하게 만들어 '결정 트리의 숲'을 만든다.\n","- 각 결정 트리의 예측을 사용해 최종 예측을 만든다.\n","\n","먼제 랜덤 포레스트는 각 트리를 훈련하기 위한 데이터를 랜덤하게 만드는데, 이 방식이 독특하다.\n","- 우리가 입력한 훈련 데이터에서 랜덤하게 샘플을 추출하여 훈련 데이터를 만든다.\n","    - 이 때, 한 샘플이 중복되어 추출될 수 있다. --> 부트스트랩 샘플\n","        - 부트스트랩: 데이터 세트에서 중복을 허용하여 데이터를 샘플링하는 방식\n","    - 기본적으로 부트스트랩 샘플은 훈련 세트의 크기와 같게 만든다.\n","- 또한, 각 노드를 분할할 때, 전체 특성 중에서 일부 특성을 무작위로 고른 다음 이 중에서 최선의 분할을 찾는다.\n","    - RandomForestClassifier는 기본적으로 전체 특성 개수의 제곱근만큼의 특성을 고른다.\n","        - 4개의 특성이면 노드마다 랜덤하게 2개를 고르고, \n","        - 그 2개의 특성에서 최선의 분할을 선택한다.\n","    - 단, 회귀 모델인 RandomForestRegressor는 전체 특성을 모두 사용한다.\n","\n","사이킷런의 랜덤 포레스트는 기본적으로 100개의 결정 트리를 이런 방식으로 훈련한다.\n","- 그 후, 분류일 때는, 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측으로 삼는다.\n","- 회귀일 때는, 각 트리의 예측을 평균한다.\n","\n","\n","---\n","\n","랜덤 포레스트는 랜덤하게 선택한 샘플과 특성을 사용하기 때문에, 과대적합을 막아주고 검증 세트와 테스트 세트에서 안정적인 성능을 얻을 수 있다.\n","\n","그럼 와인 분류 문제에 랜덤 포레스트 분류를 적용해보자."],"metadata":{"id":"GoebBsAyKfC3"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"ra0czFjQCpKP","executionInfo":{"status":"ok","timestamp":1642744622876,"user_tz":-540,"elapsed":1176,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","wine = pd.read_csv('https://bit.ly/wine_csv_data')\n","data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n","target = wine['class'].to_numpy()\n","\n","train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","source":["랜덤 포레스트 객체를 만들고, 교차 검증을 수행해보자.\n","\n","기본적으로 100개의 트리를 만들어내기 때문에, n_jobs=-1로 두자."],"metadata":{"id":"Psf6y6jgOL3J"}},{"cell_type":"code","source":["from sklearn.model_selection import cross_validate\n","from sklearn.ensemble import RandomForestClassifier\n","\n","rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n","scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1) # 훈련 평가 점수도 확인\n","\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_gPiS5aJOYRt","executionInfo":{"status":"ok","timestamp":1642745189391,"user_tz":-540,"elapsed":3758,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"a8236f39-05df-4c2d-f770-b0c2632fa9de"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9973541965122431 0.8905151032797809\n"]}]},{"cell_type":"markdown","source":["다소 과대적합이 발생한 것 같다.\n","\n","랜덤 포레스트는 결정 트리의 앙상블이기 때문에, DecisionTreeClassifier가 제공하는 주요 매개변수를 모두 지원한다.\n","- criterion, max_depth, max_features, min_sample_split, min_impurity_decrease, min_sample_leaf 등\n","- 또한 특성 중요도도 계산할 수 있다\n","    - 이는 각 결정 트리의 특성 중요도를 취합한 값이다.\n","\n","앞서 만든 랜덤 포레스트 모델을 훈련 세트에 훈련시킨 후 특성 중요도를 출력해보자."],"metadata":{"id":"2Fb3IUrsP7qw"}},{"cell_type":"code","source":["rf.fit(train_input, train_target)\n","print(rf.feature_importances_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3P6apByUQSlF","executionInfo":{"status":"ok","timestamp":1642745427637,"user_tz":-540,"elapsed":671,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"4a5456d4-0aca-4873-84c5-2e42e6e57a37"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.23167441 0.50039841 0.26792718]\n"]}]},{"cell_type":"markdown","source":["단일 결정 트리와는 다르게, 알코올 도수와 pH의 특성 중요도가 조금 상승했다.\n","- 이는, 랜덤 포레스트 분류는, 특성의 일부를 선택하여 결정 트리를 훈련시키기 때문\n","- 그 결과 좀 더 많은 특성이 훈련에 기여할 기회를 얻는다.\n","- --> 과대적합을 줄이고 일반화 성능을 높이는 데 도움이 된다.\n","\n","---\n","\n","RandomForestClassifier은 자체적으로 모델을 평가하는 점수를 얻을 수 있다.\n","- OOB(out of bag) 샘플: 결정 트리를 훈련할 때 구성한 부트스트랩 샘플에 포함되지 않고 남은 샘플\n","- OOB 샘플을 이용하여, 부트스트랩 샘플로 훈련한 결정 트리를 평가할 수 있다. (마치 검증 세트처럼)\n","- 이 점수를 얻으려면 oob_score 매개변수를 True로 지정해야 한다. (default: False)"],"metadata":{"id":"i0rPVTOCQfoR"}},{"cell_type":"code","source":["rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n","rf.fit(train_input, train_target)\n","print(rf.oob_score_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QxFDbB5QQcqj","executionInfo":{"status":"ok","timestamp":1642745662528,"user_tz":-540,"elapsed":711,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"829b356e-9bf1-4782-b942-8dbcee6765e9"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8934000384837406\n"]}]},{"cell_type":"markdown","source":["교차 검증으로 얻은 점수와 비슷한 점수가 나왔다!\n","\n","OOB 점수를 사용하면 교차 검증을 대신할 수 있어서, 결과적으로 훈련 세트에 더 많은 샘플을 사용할 수 있다."],"metadata":{"id":"BxahZSRGRbAb"}},{"cell_type":"markdown","source":["## 엑스트라 트리"],"metadata":{"id":"qDc0xY7GRyZq"}},{"cell_type":"markdown","source":["랜덤 포레스트와 매우 비슷하게 동작한다.\n","- 기본적으로 100개의 트리를 훈련하고, 대부분의 매개변수를 공유하고, 일부 특성을 선택하여 노드를 분할하는 데 사용한다.\n","\n","그러나 랜덤 포레스트와의 차이는,\n","- 부트스트랩 샘플을 사용하지 않는다. --> 결정 트리를 만들 때 전체 훈련 세트를 사용한다.\n","- 대신 노드를 분할할 때, 가장 좋은 분할을 찾지 않고, 무작위로 분할한다.\n","    - --> DecisionTreeClassifier(splitter='random')인 결정 트리를 이용한다.\n","- 하나의 결정 트리에서 랜덤하네 노드를 분할하면 성능이 낮아지겠지만(05-2 학습문제 3에서 봤듯이)\n","- 많은 트리를 앙상블 하기 때문에, 과대적합을 막고 검증 세트의 점수를 높이는 효과가 있다."],"metadata":{"id":"VPet52l-R4vL"}},{"cell_type":"code","source":["from sklearn.ensemble import ExtraTreesClassifier\n","\n","et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n","scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\n","\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6k0IRSrpSug_","executionInfo":{"status":"ok","timestamp":1642746074298,"user_tz":-540,"elapsed":3012,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"fdad78bb-50e2-4880-8971-899109ea6aaf"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9974503966084433 0.8887848893166506\n"]}]},{"cell_type":"markdown","source":["랜덤 포레스트와 비슷한 결과를 얻었다.\n","- 이 예제는 특성이 많지 않아 두 모델의 차이가 크지 않다.\n","- 보통 엑스트라 트리가 무작위성이 크기 때문에 더 많은 결정 트리를 훈련해야 한다.\n","- 하지만, 랜덤하게 노드를 분할하기 때문에 계산속도가 빠르다는 것이 장점이다.\n","    - 결정 트리는 최적의 분할을 찾는 데 시간을 많이 소모한다. (특히 특성이 많을 때)\n","\n","엑스트라 트리도 특성 중요도를 제공하며, 회귀 버전인 ExtraTreeRegressor 클래스도 존재한다."],"metadata":{"id":"MMx27q5hS_M1"}},{"cell_type":"code","source":["et.fit(train_input, train_target)\n","print(et.feature_importances_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T6Bi5rNFTZJO","executionInfo":{"status":"ok","timestamp":1642746209484,"user_tz":-540,"elapsed":671,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"ea2eee65-da87-4810-f4fc-2b8b6421bc56"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.20183568 0.52242907 0.27573525]\n"]}]},{"cell_type":"markdown","source":["# 그래디언트 부스팅"],"metadata":{"id":"pK_XK_lhRzmY"}},{"cell_type":"markdown","source":["깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차를 보완하는 방식으로 앙상블 하는 방법 (결정 트리를 연속적으로 추가하며 손실 함수를 최소화)\n","- 사이킷런의 GradientBoostingClassifier는 기본적으로 깊이가 3인 결정 트리를 100개 사용한다.\n","    - 따라서 과대적합에 강하고, 일반화 성능이 높다.\n","- 이름에서 알 수 있듯이 경사 하강법을 사용하여 트리를 앙상블에 추가한다.\n","    - 분류에서는 로지스틱 손실 함수를 사용하고,\n","    - 회귀에서는 평균 제곱 오차 함수를 사용한다.\n","\n","경사 하강법\n","- 손실 함수(산)에서 가장 낮은 곳을 찾아 조금씩 천천히 내려가는 방법\n","    - 기존 경사 하강법: 모델의 가중치와 절편을 조금씩 바꾸기\n","    - 그래디언트 부스팅: 깊이가 얕은 결정 트리를 계속 추가하기\n","\n","이제 이를 이용하여 와인 데이터셋의 교차 검증 점수를 확인해보자."],"metadata":{"id":"-atBj732TkA3"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingClassifier\n","\n","gb = GradientBoostingClassifier(random_state=42)\n","scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n","\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W4wrNyV_YPdB","executionInfo":{"status":"ok","timestamp":1642747519758,"user_tz":-540,"elapsed":2444,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"3954edbd-f2f8-428f-b64e-ff99e4d2fc8b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8881086892152563 0.8720430147331015\n"]}]},{"cell_type":"markdown","source":["과대적합이 거의 해결되었다.\n","\n","그래디언트 부스팅은 결정 트리 개수를 늘려도 과대적합에 매우 강하다.\n","- 학습률을 증가시키고 트리의 개수를 늘리면 조금 더 성능이 향상될 수 있다."],"metadata":{"id":"KVg7S-hzYe96"}},{"cell_type":"code","source":["gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n","scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n","\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0aLUvL9mYpnU","executionInfo":{"status":"ok","timestamp":1642747764141,"user_tz":-540,"elapsed":6858,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"17340ff6-d110-4b98-d3b1-7ea0c2989b3b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9464595437171814 0.8780082549788999\n"]}]},{"cell_type":"markdown","source":["결정 트리 개수를 5배 늘리고, 학습률을 2배로 늘렸다.\n","\n","그럼에도 과대적합을 잘 억제하고 있다.\n","\n","그래디언트 부스팅은 다른 트리 모델과 마찬가지로 특성 중요도를 제공한다.\n","- 단, 랜덤 포레스트보다 일부 특성(당도)에 더 집중한다."],"metadata":{"id":"N6NVnGKnZZOd"}},{"cell_type":"code","source":["gb.fit(train_input, train_target)\n","print(gb.feature_importances_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PLvcYXz_Ztlm","executionInfo":{"status":"ok","timestamp":1642747864507,"user_tz":-540,"elapsed":2275,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"c357b60f-71bd-47b5-8574-4ae4ecd8b44c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.15872278 0.68010884 0.16116839]\n"]}]},{"cell_type":"markdown","source":["그 외의 그래디언트 부스팅의 특징으로는,\n","- subsample 매개변수의 존재 (default: 1.0)\n","    - 이를 줄이면, 훈련 세트의 일부만을 이용할 수 있다.\n","    - 마치 경사 하강법 단계마다 일부 샘플을 랜덤하게 선택하는 확률적 경사 하강법이나 미니배치 경사 하강법 처럼.\n","- 일반적으로 랜덤 포레스트보다 조금 더 높은 성능을 얻을 수 있다.\n","    - 다만 병렬 훈련이 불가하기 때문에, 훈련 속도가 좀 느리고,\n","    - 그렇기에 n_jobs 매개변수가 없다.\n","- 회귀 버전으로 GradientBoostingRegressor이 존재한다."],"metadata":{"id":"y8BntZcqaBXX"}},{"cell_type":"markdown","source":["## 히스토그램 기반 그래디언트 부스팅"],"metadata":{"id":"ciC5q9NPThlJ"}},{"cell_type":"markdown","source":["정형 데이터를 다루는 머신러닝 알고리즘 중 가장 인기가 높은 알고리즘이다.\n","\n","먼저 입력 특성을 256개의 구간으로 나눈다.\n","- 노드를 분할할 때 최적의 분할을 매우 빠르게 찾을 수 있다.\n","- 이 구간 중 하나를, 누락된 값을 위해서 사용한다.\n","    - 입력에 누락된 특성이 있더라도 이를 따로 전처리할 필요가 없다.\n","\n","HistGradientBoostingClassifier 클래스\n","- 기본 매개변수에서 안정적인 성능을 얻을 수 있다.\n","- 트리의 개수를 지정하는 n_estimators 대신에, 부스팅 반복 횟수를 지정하는 max_iter를 사용한다."],"metadata":{"id":"nIhPk2Avaosa"}},{"cell_type":"code","source":["# from sklearn.experimental import enable_hist_gradient_boosting\n","from sklearn.ensemble import HistGradientBoostingClassifier\n","\n","hgb = HistGradientBoostingClassifier(random_state=42)\n","scores = cross_validate(hgb, train_input, train_target, return_train_score=True)\n","\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VHXXVkckbRQl","executionInfo":{"status":"ok","timestamp":1642748339181,"user_tz":-540,"elapsed":2576,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"0a438aa6-5d3f-490c-8dc9-5364968650c4"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9321723946453317 0.8801241948619236\n"]}]},{"cell_type":"markdown","source":["과대적합을 잘 억제하면서 기존 그래디언트 부스팅보다 조금 더 성능이 높다.\n","\n","특성 중요도를 확인해보자."],"metadata":{"id":"useeQZxYbn2B"}},{"cell_type":"code","source":["from sklearn.inspection import permutation_importance\n","\n","hgb.fit(train_input, train_target)\n","result = permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state=42, n_jobs=-1)\n","\n","print(result.importances_mean)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z7PoLczkb62N","executionInfo":{"status":"ok","timestamp":1642748523027,"user_tz":-540,"elapsed":3139,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"3fbace4c-9f82-44ad-8136-ac1da16390c8"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.08876275 0.23438522 0.08027708]\n"]}]},{"cell_type":"markdown","source":["permutation_importance() 함수\n","- 특성을 하나씩 랜덤하게 섞어서 모델의 성능이 변화하는지를 관찰함으로서, 어떤 특성이 중요한지를 계산한다.\n","    - 섞어서 성능이 떨어지면 --> 중요한 특성\n","    - 섞어서 성능 변화가 없거나 오히려 좋아지면 --> 안중요한 특성 / 해가 되는 특성\n","- 훈련 세트뿐만 아니라 테스트 세트에도 적용할 수 있고, 사이킷런에서 제공하는 추정기 모델에 모두 사용할 수 있다.\n","- n_repeats: 랜덤하게 섞을 횟수 (default: 5)\n","- 반환\n","    - 반복하여 얻은 특성 중요도 (importances)\n","    - 평균 (importances_mean)\n","    - 표준 편차 (importances_std)\n","\n","이번에는 테스트 세트에 대해서도 특성 중요도를 계산해보자."],"metadata":{"id":"wk8XNJLvcWCL"}},{"cell_type":"code","source":["result = permutation_importance(hgb, test_input, test_target, n_repeats=10, random_state=42, n_jobs=-1)\n","\n","print(result.importances_mean)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JAjenQ7Wdfk6","executionInfo":{"status":"ok","timestamp":1642748857425,"user_tz":-540,"elapsed":1738,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"fa9b8ae1-5db7-4354-8009-a4348dcb02bd"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.05969231 0.20238462 0.049     ]\n"]}]},{"cell_type":"markdown","source":["이제 최종적인 성능을 확인해보자"],"metadata":{"id":"aBQ-6BBAdoTQ"}},{"cell_type":"code","source":["hgb.score(test_input, test_target)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JJlnc7BWdrZx","executionInfo":{"status":"ok","timestamp":1642748895371,"user_tz":-540,"elapsed":400,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"698abd97-53f4-4616-b706-4e7b9bde386f"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8723076923076923"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["2절에서 측정한 랜덤 포레스트의 정확도(0.86) 보다는 높은 점수가 나왔다."],"metadata":{"id":"7-5XkcyHdq6L"}},{"cell_type":"markdown","source":["사이킷런 말고도 히스토그램 기반 그래디언트 부스팅 알고리즘을 구현한 라이브러리가 여럿 있다.\n","\n","XGBoost\n","- 사이킷런의 cross_validate()와 함께 사용할 수도 있다.\n","- tree_method 매개변수로 'hist'로 지정하면 히스토그램 기반 그래디언트 부스팅을 사용할 수 있다."],"metadata":{"id":"dzYjKpJueA70"}},{"cell_type":"code","source":["from xgboost import XGBClassifier\n","\n","xgb = XGBClassifier(tree_method='hist', random_state=42)\n","scores = cross_validate(xgb, train_input, train_target, return_train_score=True)\n","\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xbr5d3b9ePAr","executionInfo":{"status":"ok","timestamp":1642749085731,"user_tz":-540,"elapsed":688,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"053d92e0-51a6-4a07-80b3-7b8a36c96307"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8824322471423747 0.8726214185237284\n"]}]},{"cell_type":"markdown","source":["또 다른 라이브러리는 MS의 LightGBM이 있다."],"metadata":{"id":"iI1CaoDSedhv"}},{"cell_type":"code","source":["from lightgbm import LGBMClassifier\n","\n","lgb = LGBMClassifier(random_state=42)\n","scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n","\n","print(np.mean(scores['train_score']), np.mean(scores['test_score']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cnO_CyZBeiWd","executionInfo":{"status":"ok","timestamp":1642749157000,"user_tz":-540,"elapsed":1294,"user":{"displayName":"윤도현","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXnxt6znsVev-ZLUgke_ZbLPHviOHSLB6qfl54oA=s64","userId":"13391745180797711051"}},"outputId":"b9fb891b-d65e-4670-a707-ba5c5f991e18"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9338079582727165 0.8789710890649293\n"]}]},{"cell_type":"markdown","source":["사이킷런의 HGB은 사실 LightGBM의 영향을 많이 받았다.\n","\n","끝!"],"metadata":{"id":"bTE1c-Lneu8S"}}]}